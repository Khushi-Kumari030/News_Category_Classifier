# -*- coding: utf-8 -*-
"""News_category_classifier.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19lknZzzEqRH8oQNSjHtxWY2lqEsgaBOp
"""

import os
import json
import zipfile

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

kaggle_config= json.load(open('/content/kaggle.json'))

kaggle_config.keys()

os.environ['KAGGLE_USERNAME']=kaggle_config['username']
os.environ['KAGGLE_KEY']=kaggle_config['key']

#!/bin/bash
!kaggle datasets download moazeldsokyx/bbc-news

!ls

with zipfile.ZipFile('bbc-news.zip','r') as z:
  z.extractall()

df=pd.read_csv('/content/bbc-text.csv')

df.head()

df['text'][0]

df.shape

df.info()

sns.countplot(x=df['category'])

from wordcloud import WordCloud
import re
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

nltk.download('punkt')
nltk.download('punkt_tab')
nltk.download('stopwords')

from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize
import re

def preprocess_news(text_series):
    lemmatizer = WordNetLemmatizer()
    stop_words = set(stopwords.words('english'))

    def clean_text(text):
        text = text.lower()
        text = re.sub(r"http\S+|www\S+", "", text)
        text = re.sub(r"<.*?>", "", text)
        text = re.sub(r"[^a-z\s]", "", text)
        text = re.sub(r"\s+", " ", text).strip()
        tokens = nltk.word_tokenize(text)
        tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]
        return " ".join(tokens)

    return text_series.apply(clean_text)

df['cleaned_text']= preprocess_news(df['text'])

categories = df['category'].unique()

for cat in categories:
    text_cat = df[df['category'] == cat]['cleaned_text'].str.cat(sep=" ")

    wordcloud = WordCloud(width=1000, height=500, background_color='white', max_words=100).generate(text_cat)

    plt.figure(figsize=(12, 6))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis('off')
    plt.title(f"Most Frequent Words in '{cat}' News", fontsize=16)
    plt.show()

df.head()

from sklearn.feature_extraction.text import TfidfVectorizer
tfidf_vectorizer=TfidfVectorizer(max_features=5000)
tfidf_matrix=tfidf_vectorizer.fit_transform(df['cleaned_text'])

from sklearn.preprocessing import LabelEncoder

# Initialize the encoder
label_encoder = LabelEncoder()

# Fit and transform the labels
df['label'] = label_encoder.fit_transform(df['category'])

label_mapping = {label: idx for idx, label in enumerate(label_encoder.classes_)}
print("Category to Label Mapping:", label_mapping)

df.head()

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, accuracy_score

y = df['label']

X_train, X_test, y_train, y_test = train_test_split(tfidf_matrix, y, test_size=0.2, random_state=42)

clf = LogisticRegression(max_iter=1000)
clf.fit(X_train, y_train)

y_pred = clf.predict(X_test)
print("Accuracy:", accuracy_score(y_test, y_pred))
print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))

def predict_news_category(news_text):
    # Use same cleaning steps
    cleaned_text = preprocess_news(pd.Series(news_text)).iloc[0]

    # Vectorize using trained TF-IDF
    vectorized_text = tfidf_vectorizer.transform([cleaned_text])

    # Predict
    predicted_label = clf.predict(vectorized_text)[0]

    # Decode label to original category name
    predicted_category = label_encoder.inverse_transform([predicted_label])[0]

    return predicted_category

from sklearn.preprocessing import FunctionTransformer
preprocess_transformer = FunctionTransformer(preprocess_news)

from sklearn.pipeline import Pipeline
pipeline = Pipeline([
    ('preprocess', preprocess_transformer),
    ('tfidf', TfidfVectorizer(max_features=5000)),
    ('classifier', LogisticRegression(max_iter=1000))
])

df.head()

X=df['text']
y=df['label']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

pipeline.fit(X_train,y_train)

import pickle
with open("news_pipeline.pkl", "wb") as f:
    pickle.dump(pipeline, f)

with open("label_encoder.pkl", "wb") as f:
    pickle.dump(label_encoder, f)